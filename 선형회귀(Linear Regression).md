# 선형회귀(Linear Regression)

이번 챕터에서는 머신 러닝에서 쓰이는 요엉인 `가설`, `손실 함수,` `경사 하강법`에 대한 개념과

선형 회귀에 대해서 이해해보자

## 1. 선형회귀

이전 시간에 선형 회귀에 대해서 간단하게 알아보았다. 걷는 횟수가 늘 수록, 몸무게가 줄고, 집 평수가 넓을수록 매매 가격은 비싼 경향이 있다. 이는 수학적으로 생각해보면 어떤 요인의 수치에 따라 특정 요인의 수치가 영향을 받고 있다고 말할 수 있다.

> 다른 변수의 값을 변하게 하는 변수를 x, 변수 x에 의해 값이 종속적으로 변하는 변수 y가 있다.
> 

변수 `x`는 `독립적으로` 변할수 있지만, `y`값은 x에 의해 `종속적으로` 결정된다. 이때 독립 변수 x가 1개라면 `단순 선형회귀`라고 한다.

1) 단순 선형 회귀 분석

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled.png)

위 수식은 단순 선형 회귀의 수식이다. 값 `w`는 머신 러닝에서 `가중치`(weight), 별도로 더해지는 값 `b`(bias)를 `편향` 이라고 한다. 직선의 방정식에서는 각각 직선의 `기울기와 절편을` 의미한다.

→ w와 b의 값에 따라 x와 y가 표현하는 직선은 무궁무진하다.

2) 다중 선형 회귀 분석

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%201.png)

집의 매매 가격은 집의 평수 뿐만아니라, 집의 층의 수, 방의 개수, 역과의 거리 등 영향이 있다. 이러한 다수의 요소를 가지고 집의 매매 가격을 예측할 때, y는 여전히 1개이지만, x는 여러개가 된다.

## 2. 가설(Hypothesis) 세우기

어떤 학생이 공부시간에 따라서 왼쪽 표와 같은 점수를 얻었다는 데이터가 있다. 이를 좌표 평면에 그려보면 오른쪽 그림과 같다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%202.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%203.png)

알고 있는 데이터로부터  x와 y의 관계를 유하고, 이 학생의 6시간, 7시간 공부했을 떄의 성적을 예측하고싶다. 이 때 x,y의 관계를 유추하기위해 수학적으로 세우는 식을 `가설`이라고 한다.

![W와_b가_다름.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/W%25EC%2599%2580_b%25EA%25B0%2580_%25EB%258B%25A4%25EB%25A6%2584.png)

위 그림은 w와 b의 값에 따라 달라지는 직선의 모습을 보여준다. w는 직선의 기울기이고, b는 절편이다.

아직은 방법을 모르지만, 어떤 방법을 사용하여 적절한`w`와 `y`의 값을 찾은 덕택에`y`와 `x`의 관계를 가장 잘 나타내는 직선을 위의 좌표 평면 상에서 그렸다고 한 번 가정해보자. 이 직선을 x가 6일때, 7일때 쭉쭉 이어 그린다면 이 학생의 7시간 공부했을 때의 예상 점수를 말할 수 있다. 

그저 y값을 확인함으로써.

## 3. 비용함수(Const function) : 평균 제곱 오차(MSE)

머신러닝은 w와 b를 찾기위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 w와 b를 찾아낸다.

이때 실제값과 예측값에 대한 오차에 대한 식을 `목적함수`(Objective function) 또는 `비용함수`(cost function) 또는 `손실함수`(loss function) 라고한다.

> 목적함수 : 함수의 값을 `최소화`하거나, `최대화`하거나 하는 목적을 가진 함수
값을 `최소화하려고` 하면 이를 `비용함수` 또는 `손실함수`라고 한다.
> 

`비용함수는` 예측값의 오차를 줄이는 일에 최적화 된 식이어야 한다. 회귀 문제의 경우에는 주로 `평균 제곱 오차(Mean Squared Error, MSE)`가 사용된다.

![그림3.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EA%25B7%25B8%25EB%25A6%25BC3.png)

위의 그래프에 임의의 `w의 값 13`과 임의의 `b의 값 1`을 가진 직선을 그렸습니다. 임의로 그린 직선으로 정답이 아닙니다. 이제 이 직선으로부터 서서히 w와 b의 값을 바꾸면서 정답인 직선을 찾아내야 한다. 위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여준다.

y = 13x + 1 직선이 예측한 예측값을 각각 실제값으로 부터 오차를 계산하여 표를 만들어보면

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%204.png)

과 같다. 오차는 `양수 오차`도 있고, `음수 오차`도 있으므로 모든 오차를 `제곱`하여 더하는 방법을 사용한다. `n`은 갖고 있는 `데이터의 개수`를 의미한다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%205.png)

이때 n으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있다. 이를 평균 제곱 오차(MSE)라고 한다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%206.png)

y=13x+1의 `MSE`는 52.5이다. MSE를 최소값으로 만드는 `w,b`를 찾아내는 것이 관건이다. 이를 w와 b에 의한 비용함수로 재정의 해보면 다음과 같다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%207.png)

모든 점들과의 오차가 클 수록 `MSE`는 커지며, 오차가 작아질 수록 제곱 오차는 작아진다. 그러므로 `Cost(w,b)`를 최소가 되게 만드는 `w, b`를 구하면 결과적으로 `y와 x의 관계`를 가장 잘 나타내는 직선을 그릴 수 있다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%208.png)

## 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)

`비용 함수`를 최소화하는 매개 변수인 `w와 b`를 찾기 위해 사용되는 알고리즘을 `**옵티마이저**`, 또는 `**최적화 알고리즘**`이라고 부른다.

옵티마이저를 통해 적절한 `w`와 `b`를 찾아내는 과정을 머신 러닝에서 `훈련(training)` 또는 `학습(learning)`이라고 부릅니다. 여기서는 가장 기본적인 옵티마이저 알고리즘인 `경사 하강법(Gradient Descent)`에 대해서 알아보자.

![그림4.png](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EA%25B7%25B8%25EB%25A6%25BC4.png)

위 그림에서 `노랑색은` 기울기 w가 `20`일때 `초록색은` 기울기가 `1`일 때 이다.

전에 w가 13일때와 비교했을 때 더 큰 오차를 보여주고 있다. 여기서 오차와 기울기와의 관계를 다음과 같은 그래프로 표현할 수 있다.

![선형회귀1.jpg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EC%2584%25A0%25ED%2598%2595%25ED%259A%258C%25EA%25B7%25801.jpg)

기울기가 무한대로 커질 수록 cost의 값 또한 무한대로 커지고 w가 무한대로 작아지면 cost의 값은 무한대로 커진다. 즉, 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 w를 찾는 일이다.

→ 아래로 볼록한 부분의 w값을 찾아야햐 한다.

![선형회귀2.jpg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EC%2584%25A0%25ED%2598%2595%25ED%259A%258C%25EA%25B7%25802.jpg)

기계는 임의의 랜덤값 w를 정한 뒤, 맨 아래의 볼록한 부분을 향해 점차 w를 수정해 나간다. 위 그림은 그러한 과정을 보여주며, 이를 가능하게 하는 것이 `경사 하강법` 이다. 경사 하강법은 `한 점에서의 순간 변화율` 또는 `접선에서의 기울기의 개념`을 사용한다.

![선형회귀3.jpg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EC%2584%25A0%25ED%2598%2595%25ED%259A%258C%25EA%25B7%25803.jpg)

### 경사 하강법의 아이디어

1. 비용함수를 미분하여 w에서의 접선의 기울기를 구하고,

 2. 접선의 기울기가 낮은 방향으로 w 값을 변경하고 다시 미분하고, 

1. 이 과정을 접선의 기울기가 0인 곳을 향해 w의 값을 변경하는 작업을 반복

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%209.png)

> 위의 식은 현재 w에서의 접선의 기울기와 a와 곱한 값을 현재 w에서 빼서 새로운 w의 값으로 한다는 것을 의미한다.
> 

a는 여기서 학습률learning rate)인데, **지금은 생각하지말고** `현재 w`에서 `현재 w에서의 접선의 기울기를 빼는 행위`가 어떤 의미인지 알아보자.

![선형회귀4-2.jpg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EC%2584%25A0%25ED%2598%2595%25ED%259A%258C%25EA%25B7%25804-2.jpg)

위의 그림은 접선의 기울기가 음수, 0, 양수일 떄를 보여준다.

기울기가 음수면 음수를 뺴는 것은 `“해당 값을 양수로 바꾸고 더하는 것”`이다.

결국 음수 기울기를 뺴면 `w의 값이 증가`하게 되어 결과적으로 접선의 기울기가 0인방향으로 w의 값이 조정되는 것이다.

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%2010.png)

![Untitled](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/Untitled%2011.png)

기울기가 양수면 `w의 값이 감소`하게 되는데 이는 결과적으로 기울기가 0인 방향으로 w의 값이 조정되는 것이다.

### 학습률(learning rate)란?

학습률 a는 w의 값을 변경할 때, 얼마나 크게 변경할지를 결정하여 `0과 1사이`의 값을 가진다. 직관적으로 생각했을 때 `a의 값을 크게`하면 접선의 기울기가 최소가 되는 w를 빠르게 찾을 수 있을 것 같지만 그렇지 않다.

![선형회귀5.jpg](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Linear%20Regression)%2067832c9c6ada45a1860d3f805748fee8/%25EC%2584%25A0%25ED%2598%2595%25ED%259A%258C%25EA%25B7%25805.jpg)

위 그림은 a가 지나치게 높은 값을 가질 때 위 그림과 같이 w의 값이 발산하는 상황을 보여준다.

또한 a가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 a를 찾아내는 것도 중요하다.