# 로지스틱 회귀(Logistic Regression)

로지스틱 회귀 알고리즘 : 둘 중 하나를 결정하는 문제(이진분류)를 해결하기 위한 대표적인 알고리즘

# 1. 이진 분류(Binary Classification)

이전 포스트에서 주어진 데이터로부터 가중치 w와 편향 b를 찾아 데이터를 가장 잘 표현할 수 있는 직선을 찾았다.

하지만 이진 분류 문제는 직선으로 표현하는 것이 적절하지 않다. 지금 부터 그 이유에 대해서 살펴 보자.

![스크린샷 2023-02-23 오후 1.59.56.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.59.56.png)

![스크린샷 2023-02-23 오후 2.00.11.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.00.11.png)

왼쪽의 데이터로부터 합격을 1, 불합격을 0이라고 하였을 때 그래프를 그리면 오른쪽 그래프와 같다.

이러한 점들을 표현하는 그래프는 알파벳의 S자로 표현되는데, **x와 y의 관계를 표현하기 위해서**는 직선을 표현하는 함수가 아니라 **S자 형태를 표현할 수 있는 함수가 필요**하다. → `직선이 분류문제에 부적합한 1번 째 이유`

위의 예제의 경우 y가 0 또는 1 이라는 두가지 값만을 가지므로, 예측 값은 0과 1사이의 값을 가지도록 하자.

0과 1사이의 값을 확률로 해석하면 문제를 풀기가 용이하다. 만약 0.5보다 작으면 0 크면 1로 예측했다고 판단하고, **직선을 사용할 경우 y의 값이 음의 무한대부터 양의 무한대와 같은 킅 수들도 가질 수 있기 때문**에 직선이 분류문제에 부적합하다. → `직선이 분류문제에 부적합한 2번 째 이유`

# 2. 시그모이드 함수(Sigmoid function)

**출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수**

시그모이드 함수는 σ로 축약해서 표현하기도 한다. 로지스틱 회귀를 풀기위한 가설을 세워보자

![스크린샷 2023-02-23 오후 2.11.05.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.11.05.png)

e는 자연상수이다. 여기서 구해야 할 것 은 주어진 데이터에 가장 적합한 가중치w와 편향b 이다. 

![스크린샷 2023-02-23 오후 2.52.05.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.52.05.png)

![스크린샷 2023-02-23 오후 2.52.16.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.52.16.png)

![스크린샷 2023-02-23 오후 2.53.25.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.53.25.png)

첫 번째 그래프는 w는 1, b는 0임을 가정한 그래프이다.

두번째 그림에서 빨간 선은 w가 0.5일 때, 파란선은 w가 2일때의 그래프이다. w 크기와 경사는 비례한다.

세 번째 그림은 b의 값에 따라 그래프가 어떻게 변하는지 보여준다.

# 3. 비용함수(Cost function)

로지스틱 회귀 또한 `경사 하강법`을 사용하여 가중치 w를 찾아내지만, 비용 함수로는 `MSE`를 사용하지 않는다.

좋지 않은 로컬 미니멈에 빠질 가능성이 높기 때문이다.

![스크린샷 2023-02-23 오후 2.58.17.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.58.17.png)

`로지스틱 회귀`에서 `MSE`를 비용함수로 사용하면 `경사 하강법`을 사용하였을 때, 최소값이 아닌 잘못된 최소값에 빠질 가능성이 높다. 이를 전체 함수에 걸쳐 최소값인 **`글로벌 미니멈`**이 아닌 특정 구역에서의 최소값인 `로컬 미니멈`에 도달했다고 한다.

## Fomula

로지스틱 회귀라는 문제에서 가중치 w를 최소로 만드는 적절한 새로운 비용함수를 찾아야한다.

가중치를 최소화하는 아래의 어떤 함수를 `목적 함수`라고 하자. J는 목적 함수를 의미한다.

![스크린샷 2023-02-23 오후 3.03.14.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.03.14.png)

n : 샘플 데이터의 개 수 

함수 f : 실제 값 y와 예측 값 H(x)의 오차를 나타내는 함수

목적 함수는 함수 f의 평균을 계산하고 있다. 결과적으로 적절한 가중치를 찾기위해 실제값과 예측값에 대한 오차를 줄여야 하므로 f는 비용함수라고 할 수 있다. 식을 다시 쓰면

![스크린샷 2023-02-23 오후 3.05.32.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.05.32.png)

시그모이드 함수는 0과 1사이의 y값을 반환한다. 

이는 **실제값이 0일 때 y의 값이 1에 가까워지면 오차가 커지며**, **실제값이 1일때 y의 값이 0에 가까워지면 오차가 커짐**을 의미한다. 이를 반영할 수 있는 함수는 `로그 함수`를 통해 표현 가능하다.

![스크린샷 2023-02-23 오후 3.07.32.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.07.32.png)

**실제값 y가 1일 때의 그래프 : 파란색**

**실제값 y가 0일 때의 그래프 : 빨간색**

![스크린샷 2023-02-23 오후 3.08.15.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.08.15.png)

### 간단 설명

: 실제값이 1일 때, 예측값(H(x))이 1이면 오차가 0이므로 cost가 0이된다. 반면 실제값이 1일때 예측값이 0이되면 cost는 무한대로 발산한다. 

![스크린샷 2023-02-23 오후 3.10.40.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.10.40.png)

자세히 보면 y와 (1-y)가 식 중간에 들어갔고, 두개의 식을 -로 묶은 것 외에는 기존의 두 식이 들어가있는 것을 볼 수 있다. y가 0이면, ylogH(x)가 없어지고, y가 1이면, (1-y)log(1-H(x))가 없어지는데 이는 각각 y가 1일 때와 y가 0일 때의 앞서 본 식과 동일하다.

## 결과

![스크린샷 2023-02-23 오후 3.13.58.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1(Logistic%20Regression)%20267bf1f5583f47909aa3560e44d6abbf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-02-23_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.13.58.png)

로지스틱 회귀에서 찾아낸 비용 함수를 `크로스 엔트로피(Cross Entropy)함수`라고 한다. 결론적으로 로지스틱 회귀는 비용 함수로 `크로스 엔트로피 함수`를 사용하며, `가중치`를 찾기위해 `크로스 엔트로피 함수`의 `평균을` 취한 함수를 사용한다. `크로스 엔트로피 함수`는 `소프트맥스 회귀의 비용 함수이기도` 하므로 뒤에서 재언급 한다.

## 로지스틱 회귀 실습

[https://wikidocs.net/111476](https://wikidocs.net/111476)

## 다중 입력에 대한 실습

[https://wikidocs.net/35821](https://wikidocs.net/35821)